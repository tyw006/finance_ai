{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "Have your own .env file in the same directory as this notebook to use with load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "from typing import List, Optional, Dict\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URLs for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_pf_url = \"https://www.reddit.com/r/personalfinance/wiki/index/\"\n",
    "elements = partition_html(url=reddit_pf_url)\n",
    "links = []\n",
    "\n",
    "for element in elements:\n",
    "    if element.metadata.link_urls: # if element contains a link\n",
    "        relative_link = element.metadata.link_urls[0] #link_urls will be a list\n",
    "        if (\"https:\" in relative_link) & (relative_link not in links):\n",
    "            links.append(relative_link + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links1 = list(set(links[2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access links and save to local(will take a while due to rate limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_ = links1.copy()\n",
    "while links_:\n",
    "    for link in links_:\n",
    "        # Trying to avoid rate limit \n",
    "        time.sleep(30)\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            with open(rf\"pf_wiki_docs/{link.split('/')[-1]}\", \"w\") as json_file:\n",
    "                json.dump(json_data, json_file)\n",
    "            print(f\"{link} saved.\")\n",
    "            links1.remove(link)\n",
    "        # In case of rate limit blocking a url\n",
    "        else:\n",
    "            print(f\"Failed to save {link} at \", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Documents and Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_url = \"https://www.reddit.com/r/personalfinance/wiki/\"\n",
    "directory = r'YOUR_DIRECTORY_HERE'\n",
    "docs = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    # Check if the file is a regular file (not a directory)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            if 'data' in data:\n",
    "                doc = Document(page_content=data['data']['content_md'], metadata={'source': reddit_url + filename.split('.')[0]})\n",
    "                docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding model and upload chunks to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings. Choosing voyage-large-2-instruct because it is currently #2 on Huggingface MTEB for retrieval average and embedding dimensions is less\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "VOYAGE_API_KEY=os.environ['VOYAGE_API_KEY']\n",
    "\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "    voyage_api_key=VOYAGE_API_KEY,\n",
    "    model = \"voyage-large-2-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "# for local\n",
    "local_url=\"localhost:6333\"\n",
    "#cloud cluster\n",
    "cloud_url = \"YOUR CLOUD URL HERE\"\n",
    "qdrant_api_key = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    split_docs,\n",
    "    embeddings,\n",
    "    url=cloud_url,\n",
    "    api_key=qdrant_api_key,\n",
    "    prefer_grpc=True,\n",
    "    collection_name=\"reddit personal finance wiki\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vector Database retriever with contextual compression(filtering on chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding contextual compression to reduce redundant documents from being retrieved\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=0, separator=\". \") # Will split retrieved documents into smaller chunks to check for relevancy\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.6) # May not need since LLM will be evaluating this later on\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, \n",
    "                  relevant_filter\n",
    "                  ]\n",
    ")\n",
    "# Original retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    # search_type=\"mmr\",\n",
    "    search_kwargs={'k':4},\n",
    "    # lambda_mult=1,\n",
    "    return_source_documents=True)\n",
    "# Contextual compression retriever\n",
    "# using lLm to filter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "groq = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=os.environ['GROQ_API_KEY'])\n",
    "_filter = LLMChainFilter.from_llm(groq)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how much money should i be making at age 32?\"\n",
    "docs = compression_retriever.invoke(question)\n",
    "docs_clean = [Document(\n",
    "    page_content=doc.page_content,\n",
    "    metadata=doc.metadata) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Test Components for Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field \n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "class websearch(BaseModel):\n",
    "    \"\"\"\n",
    "    This tool is used to search the internet for questions that are unrelated to personal finance.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vector store that contains documents relating to personal finance.\n",
    "    Topics range from emergency funds, student loans, 401K, to paying down debt and buying a home and more.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "COHERE_API_KEY=os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "preamble_route = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "            The vectorstore contains documents relating to personal finance topics. \n",
    "            For questions related to personal finance, use the vectorstore. \n",
    "            If you do not know the answer, or the user is requesting for more recent data, use web search.\"\"\"\n",
    "\n",
    "llm = ChatCohere(model='command-r-plus',temperature=0, cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "llm_router = llm.bind_tools(tools=[websearch, vectorstore], preamble=preamble_route)\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade retrieval from vector db\n",
    "class RetrievalGrader(BaseModel):\n",
    "    \"\"\"Checking that the retrieved documents are related to the question/query.\n",
    "    Score is 'yes' or 'no'.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "#llm is already defined in a cell above\n",
    "preamble_grader = \"\"\"You are a grader evaluating relevance of the retrieved documents to the user's question.\n",
    "         If the document contains keyword(s) or a semantic meaning related to the user's question, grade it as relevant.\n",
    "         However, if the user is requesting recent information such as from the current year, then grade the retrieval as \"no\"\n",
    "         because the documents are from the past.\n",
    "         The score that should be given is either 'yes' or 'no' to show that the document is relevant to the question\"\"\"\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(RetrievalGrader, preamble=preamble_grader)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"\"\"Is the following document related to my question?\n",
    "        {document}\n",
    "        Question: {question}\"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_grader.invoke({\"question\": question,\n",
    "                         \"document\": docs_clean})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "preamble= \"\"\"You are an an expert financial advisor that uses the following documents to answer questions.\n",
    "Keep the answer as concise as possible, a maximum of three senteces.\"\"\"\n",
    "\n",
    "llm_gen = llm.bind(preamble=preamble)\n",
    "\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \",\n",
    "            additional_kwargs={\"documents\": x[\"documents\"]}\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = prompt | llm_gen | StrOutputParser()\n",
    "\n",
    "answer = rag_chain.invoke({\"documents\": docs_clean, \"question\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM fallback. If query completely unrelate to vector or websearch\n",
    "\n",
    "preamble = \"\"\"You are an assistant that graciously answers questions.\n",
    "If you don't know the answer, say I don't know.\"\"\"\n",
    "\n",
    "llm_fb = llm.bind(preamble=preamble)\n",
    "\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fallback_chain = prompt | llm_fb | StrOutputParser()\n",
    "\n",
    "question1 = \"How are you doing today?\"\n",
    "answer = fallback_chain.invoke({\"question\": question1})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading Hallucinations\n",
    "class HallucinationGrader(BaseModel):\n",
    "    \"\"\"Checking that there are no hallucinations in the generated answer.\n",
    "    Score is 'yes' or 'no'.\n",
    "    \"\"\"\n",
    "    binary_score: str = Field(description=\"If Answer is grounded in the facts, score is 'yes'. Else it is 'no'.\")\n",
    "\n",
    "preamble = \"\"\"You are an evaluator assessing whether the answer provided is grounded in or supported by retrieved documents.\n",
    "You only have to reply in 'yes' or 'no'. 'Yes' means the answer is grounded in and supported by the retrieved documents.\n",
    "'No' means the answer is not grounded in the retrieved documents.\n",
    "\"\"\"\n",
    "\n",
    "hallucination_grader = llm.with_structured_output(HallucinationGrader, preamble=preamble)\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"\"\"Retrieved Documents:\n",
    "         {documents}\n",
    "         Generated answer: {generation}\"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_chain = hallucination_prompt | hallucination_grader\n",
    "hallucination_chain.invoke({\"documents\": docs_clean, \"generation\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer grader\n",
    "\n",
    "class AnswerGrader(BaseModel):\n",
    "    \"\"\"Checking that the generated answer addresses the question.\n",
    "    Score is 'yes' or 'no'.\"\"\"\n",
    "    binary_score: str = Field(description=\"If the answer addresses the question, score is 'yes'. Else,  it is 'no'\")\n",
    "\n",
    "preamble = \"\"\"You are a grader that will give a \"yes\" or \"no\" score if the LLM generated answer resolves the given question.\n",
    "Yes means that the LLM generated answer addresses the question.\"\"\"\n",
    "\n",
    "answer_llm_grader = llm.with_structured_output(AnswerGrader, preamble=preamble)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"\"\"User question:\n",
    "         {question}\n",
    "         LLM generated answer: {generation}\"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_chain = answer_prompt | answer_llm_grader\n",
    "answer_chain.invoke({\"question\":question, \"generation\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# good because has invoke function, optimized for LLM and RAG\n",
    "web_search_tool = TavilySearchResults(api_key=os.environ['TAVILY_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture flow in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Graph state\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\" The state of the graph.\n",
    "    Attributes:\n",
    "    question: question\n",
    "    generation: LLM generation\n",
    "    documents: list of documents\"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Routing node\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or vector database.\n",
    "    Args:\n",
    "        state(dict): Current graph state. Graph state defined in class.\n",
    "    Returns:\n",
    "        str: next node to call\n",
    "    \"\"\"\n",
    "    preamble = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "            The vectorstore contains documents relating to personal finance topics. \n",
    "            For questions related to personal finance, use the vectorstore. \n",
    "            If you do not know the answer, or the user is requesting for more recent data,\n",
    "            use web search.\"\"\"\n",
    "    llm_router = llm.bind_tools(tools=[websearch, vectorstore], preamble=preamble)\n",
    "\n",
    "    route_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "    question_router = route_prompt | llm_router\n",
    "    print(\">>>ROUTE QUESTION\")\n",
    "    question = state[\"question\"]\n",
    "    decision = question_router.invoke({\"question\": question})\n",
    "    # Fallback to LLM if no decision\n",
    "    if \"tool_calls\" not in decision.additional_kwargs:\n",
    "        print(\">>>ROUTE QUESTION TO LLM\")\n",
    "        return \"llm_fallback\"\n",
    "    if len(decision.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "        raise \"Router could not decide source.\"\n",
    "    # Choose route\n",
    "    route = decision.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "    if route == 'websearch':\n",
    "        print(\">>>ROUTE QUESTION TO WEB SEARCH\")\n",
    "        return \"web_search\"\n",
    "    elif route =='vectorstore':\n",
    "        print(\">>>ROUTE QUESTION TO RAG\")\n",
    "        return \"vectorstore\"\n",
    "    else:\n",
    "        print(\">>>ROUTE QUESTION TO LLM\")\n",
    "        return \"llm_fallback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM fallback node\n",
    "def llm_fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    preamble = \"\"\"You are an assistant that graciously answers questions.\n",
    "                Answer any question presented to you.\n",
    "                If you don't know the answer, say I don't know.\"\"\"\n",
    "\n",
    "    llm_fb = llm.bind(preamble=preamble)\n",
    "\n",
    "    prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                f\"Question: {x['question']} \\nAnswer: \"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fallback_chain = prompt | llm_fb | StrOutputParser()\n",
    "    print(\">>>LLM Fallback\")\n",
    "    question = state[\"question\"]\n",
    "    generation = fallback_chain.invoke({\"question\": question})\n",
    "    return {\"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define web search node\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\">>>WEB SEARCH\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    sources = \"\\n\".join([d['url'] for d in docs])\n",
    "    web_results = Document(page_content=web_results, metadata={'source': sources})\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector db retrieval node\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\">>>RETRIEVE\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = compression_retriever.invoke(question)\n",
    "    # Keep only page content and metadata of documents\n",
    "    docs_clean = [Document(\n",
    "    page_content=doc.page_content,\n",
    "    metadata=doc.metadata) for doc in documents]\n",
    "    return {\"documents\": docs_clean, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define retrieval grader node\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\">>>CHECK DOCUMENT RELEVANCE TO QUESTION\")\n",
    "    preamble = \"\"\"\n",
    "    You are a grader evaluating relevance of the retrieved documents to the user's question.\n",
    "    If the document contains keyword(s) or a semantic meaning related to the user's question,\n",
    "    grade it as relevant. The score that should be given is either 'yes' or 'no' to show that \n",
    "    the document is relevant to the question\n",
    "    \"\"\"\n",
    "    structured_llm_grader = llm.with_structured_output(RetrievalGrader, preamble=preamble)\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"\"\"Is the following document related to my question?\n",
    "            {document}\n",
    "            Question: {question}\"\"\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\">>>GRADE: DOCUMENT RELEVANT\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\">>>GRADE: DOCUMENT NOT RELEVANT\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decide to generate node\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\">>>ASSESS GRADED DOCUMENTS\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\">>>DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\">>>DECISION: GENERATE\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define answer generation node\n",
    "def generate_answer(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\">>>GENERATE\")\n",
    "\n",
    "    preamble= \"\"\"You are an an expert financial advisor that uses the following documents to answer questions.\n",
    "    If you don't know the answer, just say \"I don't know.\"\n",
    "    Keep the answer as concise as possible, a maximum of three senteces.\"\"\"\n",
    "\n",
    "    llm_gen = llm.bind(preamble=preamble)\n",
    "\n",
    "    prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                f\"Question: {x['question']} \\nAnswer: \",\n",
    "                additional_kwargs={\"documents\": x[\"documents\"]}\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    rag_chain = prompt | llm_gen | StrOutputParser()\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    if not isinstance(documents, list):\n",
    "        documents = [documents]\n",
    "\n",
    "    # RAG generation\n",
    "    answer = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# Define hallucination/answer grader node\n",
    "def hallucination_and_answer_grader(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\">>>CHECKING HALLUCINATIONS\")\n",
    "    # Define hallucination grading chain\n",
    "    preamble = \"\"\"You are an evaluator assessing whether the answer provided is grounded in or supported by retrieved documents.\n",
    "    You only have to reply in 'yes' or 'no'. 'Yes' means the answer is grounded in and supported by the retrieved documents.\n",
    "    'No' means the answer is not grounded in the retrieved documents.\n",
    "    \"\"\"\n",
    "\n",
    "    hallucination_grader = llm.with_structured_output(HallucinationGrader, preamble=preamble)\n",
    "\n",
    "    hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"\"\"Retrieved Documents:\n",
    "            {documents}\n",
    "            Generated answer: {generation}\"\"\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hallucination_chain = hallucination_prompt | hallucination_grader\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_chain.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    \n",
    "    grade = score.binary_score\n",
    "    \n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\">>>DECISION: GENERATION IS GROUNDED IN DOCUMENTS\")\n",
    "        # Check question-answering\n",
    "        print(\">>>GRADE GENERATION vs QUESTION\")\n",
    "        # Define answer grader chain\n",
    "        preamble1 = \"\"\"You are a grader that will give a \"yes\" or \"no\" score if the LLM generated answer \n",
    "        resolves the given question.\n",
    "        Yes means that the LLM generated answer addresses the question.\"\"\"\n",
    "\n",
    "        answer_llm_grader = llm.with_structured_output(AnswerGrader, preamble=preamble1)\n",
    "\n",
    "        answer_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"human\", \"\"\"User question:\n",
    "                {question}\n",
    "                LLM generated answer: {generation}\"\"\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer_chain = answer_prompt | answer_llm_grader\n",
    "\n",
    "        score = answer_chain.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        \n",
    "        if grade == \"yes\":\n",
    "            print(\">>>DECISION: GENERATION ADDRESSES QUESTION\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\">>>DECISION: GENERATION DOES NOT ADDRESS QUESTION\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\">>>DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, USE WEB SEARCH\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "# For chat memory\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\") # can replace :memory: with database path\n",
    "\n",
    "# Build Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define nodes\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback)\n",
    "\n",
    "# Set start point of graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question, # function calls what to do next\n",
    "    {\"web_search\": \"web_search\", # mapping output of route_question to node\n",
    "     \"vectorstore\": \"retrieve\",\n",
    "     \"llm_fallback\": \"llm_fallback\"}\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"generate_answer\") # web_search node routes to \"generate\"\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\") # retrieving document goes to document grader\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate, # function that outputs web_search again or generate\n",
    "    {\"web_search\": \"web_search\",\n",
    "     \"generate\":\"generate_answer\"\n",
    "     }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_answer\",\n",
    "    hallucination_and_answer_grader,\n",
    "    {\"re-generate\": \"generate_answer\",\n",
    "     \"not useful\": \"web_search\",\n",
    "     \"useful\":END\n",
    "     }\n",
    ")\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "# assemble all nodes and edges\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Langsmith for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# .env file contains \"LANGCHAIN_API_KEY\"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'personal finance adaptive RAG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"Should I invest in cryptocurrency?\"}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for output in app.stream(inputs, config):\n",
    "    for key, value in output.items():\n",
    "        # pprint.pprint(f'Node {key}: {value}')\n",
    "        None\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"I'm feeling risky. Recommend me one to invest in.\"}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for output in app.stream(inputs, config):\n",
    "    for key, value in output.items():\n",
    "        # pprint.pprint(f'Node {key}: {value}')\n",
    "        None\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
